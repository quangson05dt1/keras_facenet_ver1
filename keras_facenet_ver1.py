# -*- coding: utf-8 -*-
"""keras_facenet_ver1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/quangson05dt1/852cf231813411d6c2a7e5bf7adca3a4/keras_facenet_ver1.ipynb

# . References:
<font color='blue' font-family= "Times New Roman">
<p>This project using FaceNet to face recognition. FaceNet was proposed by <a href="http://www.florian-schroff.de">Florian Schroff</a> Florian Schroff in the 2015 article <a href="https://arxiv.org/abs/1503.03832">FaceNet: A Unified Embedding for Face Recognition and Clustering</a>
</p>
<p>
Pretrained FaceNet model was created by <a href="https://drive.google.com/drive/folders/1pwQ3H4aJ8a6yyJHZkTwtjcL4wYWQb7bn">Hiroki Taniai</a> download here.</p>

Download model and put to the folder: 'input/facenet-keras/facenet_keras.h5'

    input
    ├── facenet-keras
    │   └── facenet_keras.h5
    ├── faces-dataset
    │   └── data
    ├── unknown
    │   ├── 1.jpg
    │   ├── 2.jpg
    │   ├── ...
    │   └── ...
    Output

# 1. Colect data:

input
    ├── facenet-keras
    │   └── facenet_keras.h5
    ├── faces-dataset
    │   └── data
    ├── unknown
    │   ├── 1.jpg
    │   ├── 2.jpg
    │   ├── ...
    │   └── ...
    Output

Copy data set image to 2 folders 'train' and 'val' in the parent folder 'data'. Each folder 'train' and 'val' has the following structure:

    train
    ├── ChiPu
    │   ├── ChiPu_0001.jpg
    │   ├── ChiPu_0002.jpg
    │   ├── ...
    │   ├── ChiPu_0014.jpg
    │   └── ChiPu_0015.jpg
    ├──....
    ├── Quang Son
    │   ├── Quang Son_0001.jpg
    │   ├── Quang Son_0002.jpg
    │   ├── ...
    │   ├── Quang Son_0014.jpg
    │   └── Quang Son_0015.jpg
    ├── ...
    │   
    ├── TruongGiang
    │   ├── TruongGiang_0001.jpg
    │   ├── TruongGiang_0002.jpg
    │   ├── ...
    │   ├── TruongGiang_0015.jpg
    │   └── TruongGiang_0016.jpg

# 2. Requirements:
"""

!pip install mtcnn
!pip install tensorflow<2.0
!pip install scipy==1.1.0
!pip install scikit-learn
!pip install opencv-python
!pip install h5py
!pip install matplotlib
!pip install Pillow
!pip install requests
!pip install psutil

import mtcnn
# print version
print(mtcnn.__version__)

#Run at Google colab.
from google.colab import drive
drive.mount("/content/drive")
!ls "drive/My Drive/Colab Notebooks/"

import os

##Run at Google colab.: copy database copy data to VM of colab
!cp -r "drive/My Drive/Colab Notebooks/facenet.zip" "/content"
!cp -r "drive/My Drive/Colab Notebooks/faces-dataset.npz" "/content"

##Run at Google colab: unzip database
!unzip facenet.zip

##Run at Google colab
!ls

##Run at Google colab
!ls "faces-dataset/data/train"
!ls "faces-dataset/data/val"

"""# 3. Pre-processing

## Face alignment using MTCNN

Using function: <code>extract_face(filename)</code>, return the faces from input image file, input image normalization: 160x160.

Open source https://pypi.org/project/mtcnn/

You can refer to the method here: 
https://towardsdatascience.com/face-detection-neural-network-structure-257b8f6f85d1
https://medium.com/dummykoders/face-detection-using-mtcnn-part-1-c35c4ad9c542
https://towardsdatascience.com/mtcnn-face-detection-cdcb20448ce0

<img src="https://miro.medium.com/max/511/1*4yubqhjXWyaENvbqdK-zJA.png">

# Pre-Train: facenet

Using function <code>get_embedding(facenet_model, faces)</code> in that model <code>facenet_model</code> has been pretrain before. faces obtained from pre-processing in MTCNN

Open source https://github.com/davidsandberg/facenet

Referance articles: https://arxiv.org/abs/1503.03832

<img src="https://miro.medium.com/max/700/1*OmFw4wZx5Rx3w4TpB7hS-g.png">

# 4. Train models
Using SVC model of scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html

<img src="https://chrisalbon.com/images/machine_learning_flashcards/Support_Vector_Classifier_print.png">
"""

# This Python 3 environment comes with many helpful analytics libraries installed
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import cv2 # opencv
from mtcnn.mtcnn import MTCNN
from matplotlib import pyplot as plt
from keras.models import load_model
from PIL import Image

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import os
print(os.listdir('input'))
# Any results you write to the current directory are saved as output.

train_faces_dataset = 'input/faces-dataset/data/train/'
test_faces_dataset = 'input/faces-dataset/data/val/'

# extract a single face from a given photograph
def extract_face(filename, required_size=(160, 160)):
    # load image from file
    #image = Image.open(filename)
    image = cv2.imread(filename)
    # convert to RGB, if needed
    #image = image.convert('RGB')
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    # convert to array
    pixels = np.asarray(image)
    # create the detector, using default weights
    detector = MTCNN()
    # detect faces in the image
    results = detector.detect_faces(pixels)
    # extract the bounding box from the first face
    x1, y1, width, height = results[0]['box']
    # deal with negative pixel index
    x1, y1 = abs(x1), abs(y1)
    x2, y2 = x1 + width, y1 + height
    # extract the face
    face = pixels[y1:y2, x1:x2]
    # resize pixels to the model size
    image = Image.fromarray(face)
    image = image.resize(required_size)
    face_array = np.asarray(image)
    return face_array
def load_face(dir):
    faces = list()
    # enumerate files
    for filename in os.listdir(dir):
        path = dir + filename
        face = extract_face(path)
        faces.append(face)
    return faces

def load_dataset(dir):
    # list for faces and labels
    X, y = list(), list()
    for subdir in os.listdir(dir):
        path = dir + subdir + '/'
        faces = load_face(path)
        labels = [subdir for i in range(len(faces))]
        # print progress
        print("loaded %d sample for class: %s" % (len(faces),subdir) ) 
        X.extend(faces)
        y.extend(labels)
    return np.asarray(X), np.asarray(y)

# load the photo and extract the face
pixels = extract_face(train_faces_dataset + 'TrucAnh/TrucAnh_0001.jpg')
plt.imshow(pixels)
plt.show()
print(pixels.shape)

# load train dataset
trainX, trainy = load_dataset(train_faces_dataset)
print(trainX.shape, trainy.shape)
# load test dataset
testX, testy = load_dataset(test_faces_dataset)
print(testX.shape, testy.shape)

# save and compress the dataset for further use
np.savez_compressed('faces-dataset.npz', trainX, trainy, testX, testy)

#Sử dụng khi ở colab. copy database được pre-processing bằng MTCNN đã nén lại để sử dụng sau này
#giảm thời gian xử lý dữ liệu, cope lại GG Drive
!cp -r "/content/faces-dataset.npz" "drive/My Drive/Colab Notebooks"

# load the face dataset đã được xử lý trước đó
data = np.load('input/faces-dataset.npz')
#Load train, test data
trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']
print('Loaded: ', trainX.shape, trainy.shape, testX.shape, testy.shape)

# load the facenet model
facenet_model = load_model('input/facenet-keras/facenet_keras.h5')
print('Loaded Model')

def get_embedding(model, face):
    # scale pixel values
    face = face.astype('float32')
    # standardization
    mean, std = face.mean(), face.std()
    face = (face-mean)/std
    # transfer face into one sample (3 dimension to 4 dimension)
    sample = np.expand_dims(face, axis=0)
    # make prediction to get embedding
    yhat = model.predict(sample)
    return yhat[0]
    
# convert each face in the train set into embedding
emdTrainX = list()
for face in trainX:
    emd = get_embedding(facenet_model, face)
    emdTrainX.append(emd)
    
emdTrainX = np.asarray(emdTrainX)
print(emdTrainX.shape)

# convert each face in the test set into embedding
emdTestX = list()
for face in testX:
    emd = get_embedding(facenet_model, face)
    emdTestX.append(emd)
emdTestX = np.asarray(emdTestX)
print(emdTestX.shape)

# save arrays to one file in compressed format
np.savez_compressed('faces-embeddings.npz', emdTrainX, trainy, emdTestX, testy)

#Sử dụng cho colab, sau khi emdedded các tập data, nén dữ liệu lại để lưu dùng cho lần sau, copy sang GG Drive
!ls
!cp -r "/content/faces-embeddings.npz" "drive/My Drive/Colab Notebooks"

#Sau khi train bằng face-net, sử dụng phương pháp SVC để phân loại các hình ảnh, đưa ra label chính xác
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import Normalizer
from sklearn.svm import SVC

print("Dataset: train=%d, test=%d" % (emdTrainX.shape[0], emdTestX.shape[0]))
# normalize input vectors
in_encoder = Normalizer()
emdTrainX_norm = in_encoder.transform(emdTrainX)
emdTestX_norm = in_encoder.transform(emdTestX)
# label encode targets
out_encoder = LabelEncoder()
out_encoder.fit(trainy)
trainy_enc = out_encoder.transform(trainy)
testy_enc = out_encoder.transform(testy)
# fit model
model = SVC(kernel='linear', probability=True)
model.fit(emdTrainX_norm, trainy_enc)
# predict
yhat_train = model.predict(emdTrainX_norm)
yhat_test = model.predict(emdTestX_norm)
# score
score_train = accuracy_score(trainy_enc, yhat_train)
score_test = accuracy_score(testy_enc, yhat_test)
# summarize
print('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))

from random import choice
# select a random face from test set
selection = choice([i for i in range(testX.shape[0])])
random_face = testX[selection]

emdTestX_norm = in_encoder.transform(emdTestX)
random_face_emd = emdTestX_norm[selection]
random_face_class = testy_enc[selection]
random_face_name = out_encoder.inverse_transform([random_face_class])

# prediction for the face
samples = np.expand_dims(random_face_emd, axis=0)
yhat_class = model.predict(samples)
yhat_prob = model.predict_proba(samples)
# get name
class_index = yhat_class[0]
class_probability = yhat_prob[0,class_index] * 100
predict_names = out_encoder.inverse_transform(yhat_class)
all_names = out_encoder.inverse_transform([0,1,2,3,4,5,6,7,8,9,10,11,12,13])

print('Predicted: %s (%.3f)' % (predict_names[0], class_probability))
print('Predicted: \n%s \n%s' % (all_names, yhat_prob[0]*100))
print('Expected: %s' % random_face_name[0])
# plot face
plt.imshow(random_face)
title = '%s (%.3f)' % (predict_names[0], class_probability)
plt.title(title)
plt.show()

from matplotlib import pyplot
from matplotlib.patches import Rectangle

# draw an image with detected objects
def draw_image_with_boxes(model,filename,confidence=40):
    required_size=(160, 160)
    in_encoder = Normalizer()
    # load the image
    #data = pyplot.imread(filename)
    data = cv2.imread(filename)
    
    data = cv2.cvtColor(data, cv2.COLOR_BGR2RGB)
    # plot the image
    #pyplot.imshow(data)
    pixels = np.asarray(data)
    # get the context for drawing boxes
    #ax = pyplot.gca()
    # create the detector, using default weights
    detector = MTCNN()
    # detect faces in the image
    faces = detector.detect_faces(pixels)
    # display faces on the original image
    # plot each box
    emdfaces = list()
    for face in faces:
        # get coordinates
        x1, y1, width, height = face['box']
        # create the shape
        #rect = Rectangle((x1, y1), width, height, fill=False, color='red')
        # draw the box
        #ax.add_patch(rect)
        # deal with negative pixel index
        x1 = abs(x1)
        y1 = abs(y1)
        x2, y2 = x1 + width, y1 + height
        # extract the face
        face_ex = pixels[y1:y2, x1:x2]
        # resize pixels to the model size
        image = Image.fromarray(face_ex)
        image = image.resize(required_size)
        face_array = np.asarray(image)
        emd = get_embedding(facenet_model, face_array)
        emdfaces.append(emd)
    emdfaces = np.asarray(emdfaces)
    emdfaces_norm = in_encoder.transform(emdfaces)
    for i in range(len(faces)):
        x1, y1, width, height = faces[i]['box']
        x2, y2 = x1 + width, y1 + height
        random_face_emd = emdfaces_norm[i]
        samples = np.expand_dims(random_face_emd, axis=0)
        yhat_class = model.predict(samples)
        yhat_prob = model.predict_proba(samples)
        class_index = yhat_class[0]
        class_probability = yhat_prob[0,class_index] * 100
        predict_names = out_encoder.inverse_transform(yhat_class)
        if class_probability>confidence:
            clase_name = predict_names[0]
        else:
            clase_name = 'Unknown'
        cv2.rectangle(data, (x1, y1), (x2, y2), (255,0,0), 2)
        cv2.putText(data, clase_name, (x1, y2+30), cv2.FONT_HERSHEY_SIMPLEX, 1,(0,0,255), thickness=2, lineType=2)
        cv2.putText(data, '{:.02f}'.format(class_probability), (x1, y2+50), cv2.FONT_HERSHEY_SIMPLEX, 0.8,(255,0,255), thickness=2, lineType=2)
        print(f'{clase_name}: {class_probability:.2f}')
    #pyplot.imshow(data)
    #pyplot.savefig('output/ouputfie_.png',dpi=300)   # save the figure to file
    
    height, width = data.shape[:2]
    my_dpi = 96
    fig = pyplot.figure(figsize=(width/my_dpi, height/my_dpi), dpi=my_dpi,frameon=False)
    ax = pyplot.Axes(fig, [0., 0., 1., 1.])
    ax.set_axis_off()
    fig.add_axes(ax)
    cv2.putText(data, 'Identify Face using KERAS-FACENET', (0, 20), cv2.FONT_HERSHEY_DUPLEX, 0.8,(0,255,0), thickness=1, lineType=1)
    cv2.putText(data, 'Pham Quang Son@2020', (0, 40), cv2.FONT_HERSHEY_DUPLEX, 0.8,(255,0,0), thickness=1, lineType=1)
    ax.imshow(data)
    fig.savefig('output/ouputfie_2.png',dpi=my_dpi)

filename = 'input/unknown/6.jpg'
draw_image_with_boxes(model,filename)

import time
def draw_video_with_boxes(model,frame,confidence=40):
    required_size=(160, 160)
    in_encoder = Normalizer()
    
    data = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    # plot the image
    #pyplot.imshow(data)
    pixels = np.asarray(data)
    # get the context for drawing boxes
    #ax = pyplot.gca()
    # create the detector, using default weights
    detector = MTCNN()
    # detect faces in the image
    faces = detector.detect_faces(pixels)
    # display faces on the original image
    # plot each box
    emdfaces = list()
    for face in faces:
        # get coordinates
        x1, y1, width, height = face['box']
        # create the shape
        #rect = Rectangle((x1, y1), width, height, fill=False, color='red')
        # draw the box
        #ax.add_patch(rect)
        # deal with negative pixel index
        x1 = abs(x1)
        y1 = abs(y1)
        x2, y2 = x1 + width, y1 + height
        # extract the face
        face_ex = pixels[y1:y2, x1:x2]
        # resize pixels to the model size
        image = Image.fromarray(face_ex)
        image = image.resize(required_size)
        face_array = np.asarray(image)
        emd = get_embedding(facenet_model, face_array)
        emdfaces.append(emd)
    emdfaces = np.asarray(emdfaces)
    emdfaces_norm = in_encoder.transform(emdfaces)
    for i in range(len(faces)):
        x1, y1, width, height = faces[i]['box']
        x2, y2 = x1 + width, y1 + height
        random_face_emd = emdfaces_norm[i]
        samples = np.expand_dims(random_face_emd, axis=0)
        yhat_class = model.predict(samples)
        yhat_prob = model.predict_proba(samples)
        class_index = yhat_class[0]
        class_probability = yhat_prob[0,class_index] * 100
        predict_names = out_encoder.inverse_transform(yhat_class)
        if class_probability>confidence:
            clase_name = predict_names[0]
        else:
            clase_name = 'Unknown'
        cv2.rectangle(data, (x1, y1), (x2, y2), (255,0,0), 2)
        cv2.putText(data, clase_name, (x1, y2+30), cv2.FONT_HERSHEY_SIMPLEX, 1,(0,0,255), thickness=2, lineType=2)
        cv2.putText(data, '{:.02f}'.format(class_probability), (x1, y2+50), cv2.FONT_HERSHEY_SIMPLEX, 0.8,(255,0,255), thickness=2, lineType=2)
        print(f'{clase_name}: {class_probability:.2f}')
    #pyplot.imshow(data)
    #pyplot.savefig('output/ouputfie_.png',dpi=300)   # save the figure to file
    
    height, width = data.shape[:2]
    my_dpi = 96
    fig = pyplot.figure(figsize=(width/my_dpi, height/my_dpi), dpi=my_dpi,frameon=False)
    ax = pyplot.Axes(fig, [0., 0., 1., 1.])
    ax.set_axis_off()
    fig.add_axes(ax)
    cv2.putText(data, 'Identify Face using KERAS-FACENET', (0, 20), cv2.FONT_HERSHEY_DUPLEX, 0.8,(0,255,0), thickness=1, lineType=1)
    cv2.putText(data, 'Pham Quang Son@2020', (0, 40), cv2.FONT_HERSHEY_DUPLEX, 0.8,(255,0,0), thickness=1, lineType=1)
    return data

def run(video_file=None, output_file=None):
    frame_interval = 3  # Number of frames after which to run face detection
    fps_display_interval = 5  # seconds
    frame_rate = 0
    frame_count = 0
    if video_file is not None:
        video_capture = cv2.VideoCapture(video_file)
    else:
        # Use internal camera
        video_capture = cv2.VideoCapture(0)
    ret, frame = video_capture.read()
    width = frame.shape[1]
    height = frame.shape[0]
    if output_file is not None:
        video_format = cv2.VideoWriter_fourcc(*'XVID')
        out = cv2.VideoWriter(output_file, video_format, 20, (width, height))
    
    start_time = time.time()
    colors = np.random.uniform(0, 255, size=(1, 3))
    while True:
        # Capture frame-by-frame
        ret, frame = video_capture.read()

        frame = draw_video_with_boxes(model,frame,40)
        
        frame_count += 1
        cv2.imshow('Video', frame)
        if output_file is not None:
            out.write(frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    # When everything is done, release the capture
    if output_file is not None:
        out.release()
    video_capture.release()
    cv2.destroyAllWindows()
run('demo.mp4','out.avi')

